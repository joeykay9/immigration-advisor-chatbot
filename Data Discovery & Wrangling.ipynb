{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Name: selenium\nVersion: 3.141.0\nSummary: Python bindings for Selenium\nHome-page: https://github.com/SeleniumHQ/selenium/\nAuthor: UNKNOWN\nAuthor-email: UNKNOWN\nLicense: Apache 2.0\nLocation: c:\\users\\joel\\anaconda3\\lib\\site-packages\nRequires: urllib3\nRequired-by: \n---\nName: webdriver-manager\nVersion: 3.2.1\nSummary: Library provides the way to automatically manage drivers for different browsers\nHome-page: https://github.com/SergeyPirogov/webdriver_manager\nAuthor: Sergey Pirogov\nAuthor-email: automationremarks@gmail.com\nLicense: UNKNOWN\nLocation: c:\\users\\joel\\anaconda3\\lib\\site-packages\nRequires: configparser, requests, crayons\nRequired-by: \n---\nName: pandas\nVersion: 0.24.1\nSummary: Powerful data structures for data analysis, time series, and statistics\nHome-page: http://pandas.pydata.org\nAuthor: None\nAuthor-email: None\nLicense: BSD\nLocation: c:\\users\\joel\\anaconda3\\lib\\site-packages\nRequires: python-dateutil, pytz, numpy\nRequired-by: statsmodels, seaborn\n---\nName: beautifulsoup4\nVersion: 4.7.1\nSummary: Screen-scraping library\nHome-page: http://www.crummy.com/software/BeautifulSoup/bs4/\nAuthor: Leonard Richardson\nAuthor-email: leonardr@segfault.org\nLicense: MIT\nLocation: c:\\users\\joel\\anaconda3\\lib\\site-packages\nRequires: soupsieve\nRequired-by: conda-build\n---\nName: py2neo\nVersion: 4.3.0\nSummary: Python client library and toolkit for Neo4j\nHome-page: http://py2neo.org/\nAuthor: Nigel Small <technige@nige.tech>\nAuthor-email: py2neo@nige.tech\nLicense: Apache License, Version 2.0\nLocation: c:\\users\\joel\\anaconda3\\lib\\site-packages\nRequires: click, pytz, neobolt, colorama, urllib3, pygments, prompt-toolkit, neotime, certifi\nRequired-by: \n"
    }
   ],
   "source": [
    "# Check if project packages are installed\n",
    "! pip show selenium webdriver-manager pandas beautifulsoup4 py2neo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Requirement already satisfied: selenium in c:\\users\\joel\\anaconda3\\lib\\site-packages (3.141.0)\nRequirement already satisfied: webdriver-manager in c:\\users\\joel\\anaconda3\\lib\\site-packages (3.2.1)\nRequirement already satisfied: pandas in c:\\users\\joel\\anaconda3\\lib\\site-packages (0.24.1)\nRequirement already satisfied: beautifulsoup4 in c:\\users\\joel\\anaconda3\\lib\\site-packages (4.7.1)\nRequirement already satisfied: py2neo in c:\\users\\joel\\anaconda3\\lib\\site-packages (4.3.0)\nRequirement already satisfied: urllib3 in c:\\users\\joel\\anaconda3\\lib\\site-packages (from selenium) (1.24.2)\nRequirement already satisfied: requests in c:\\users\\joel\\anaconda3\\lib\\site-packages (from webdriver-manager) (2.22.0)\nRequirement already satisfied: configparser in c:\\users\\joel\\anaconda3\\lib\\site-packages (from webdriver-manager) (5.0.0)\nRequirement already satisfied: crayons in c:\\users\\joel\\anaconda3\\lib\\site-packages (from webdriver-manager) (0.3.1)\nRequirement already satisfied: numpy>=1.12.0 in c:\\users\\joel\\anaconda3\\lib\\site-packages (from pandas) (1.16.4)\nRequirement already satisfied: pytz>=2011k in c:\\users\\joel\\anaconda3\\lib\\site-packages (from pandas) (2019.1)\nRequirement already satisfied: python-dateutil>=2.5.0 in c:\\users\\joel\\anaconda3\\lib\\site-packages (from pandas) (2.8.0)\nRequirement already satisfied: soupsieve>=1.2 in c:\\users\\joel\\anaconda3\\lib\\site-packages (from beautifulsoup4) (1.8)\nRequirement already satisfied: click==7.0 in c:\\users\\joel\\anaconda3\\lib\\site-packages (from py2neo) (7.0)\nRequirement already satisfied: prompt-toolkit~=2.0.7 in c:\\users\\joel\\anaconda3\\lib\\site-packages (from py2neo) (2.0.9)\nRequirement already satisfied: neobolt~=1.7.12 in c:\\users\\joel\\anaconda3\\lib\\site-packages (from py2neo) (1.7.17)\nRequirement already satisfied: pygments~=2.3.1 in c:\\users\\joel\\anaconda3\\lib\\site-packages (from py2neo) (2.3.1)\nRequirement already satisfied: certifi in c:\\users\\joel\\anaconda3\\lib\\site-packages (from py2neo) (2020.4.5.1)\nRequirement already satisfied: colorama in c:\\users\\joel\\anaconda3\\lib\\site-packages (from py2neo) (0.4.1)\nRequirement already satisfied: neotime~=1.7.4 in c:\\users\\joel\\anaconda3\\lib\\site-packages (from py2neo) (1.7.4)\nRequirement already satisfied: idna<2.9,>=2.5 in c:\\users\\joel\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (2.8)\nRequirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\joel\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (3.0.4)\nRequirement already satisfied: six>=1.5 in c:\\users\\joel\\anaconda3\\lib\\site-packages (from python-dateutil>=2.5.0->pandas) (1.12.0)\nRequirement already satisfied: wcwidth in c:\\users\\joel\\anaconda3\\lib\\site-packages (from prompt-toolkit~=2.0.7->py2neo) (0.1.7)\n"
    }
   ],
   "source": [
    "# install packages if not already installed\n",
    "! pip install selenium webdriver-manager pandas beautifulsoup4 py2neo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager \n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "from py2neo import Graph, Node, Relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Web pages to be scraped\n",
    "DOCUMENTS_URL = \"https://www.gov.uk/guidance/immigration-rules\"\n",
    "POINT_BASED_SYSTEM_URL = \"https://www.gov.uk/guidance/immigration-rules/immigration-rules-part-6a-the-points-based-system\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish database connection\n",
    "\n",
    "graph = Graph('bolt://localhost:7687', auth=('neo4j', 'Undertaker11.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open documents url for scraping\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "driver.get(DOCUMENTS_URL)\n",
    "\n",
    "content = driver.page_source\n",
    "soup = BeautifulSoup(content)\n",
    "\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[WDM] - Current google-chrome version is 83.0.4103\n[WDM] - Get LATEST driver version for 83.0.4103\n \n[WDM] - Driver [C:\\Users\\Joel\\.wdm\\drivers\\chromedriver\\win32\\83.0.4103.39\\chromedriver.exe] found in cache\n"
    }
   ],
   "source": [
    "#Get points based system immigration rules(part 6a) web page\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "driver.get(POINT_BASED_SYSTEM_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "paragraph_headings = []\n",
    "paragraphs = []\n",
    "attributes = []\n",
    "rules = []\n",
    "\n",
    "content = driver.page_source\n",
    "soup = BeautifulSoup(content)\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for paragraph in soup.findAll('h2', attrs={'class': 'js-subsection-title'}):\n",
    "    if(re.search(\"^\\d\", paragraph.text)):\n",
    "        paragraph_headings.append(paragraph.text)\n",
    "\n",
    "for paragraph in soup.findAll('h3'):\n",
    "    if(re.search(\"^\\d\", paragraph.text)):\n",
    "        paragraph_headings.append(paragraph.text)\n",
    "\n",
    "# Remove deleted paragraphs from paragraph_headings list\n",
    "deleted_paragraph_headings = []\n",
    "\n",
    "for paragraph_heading in paragraph_headings:\n",
    "    if(re.search(\"DELETED$\", paragraph_heading)):\n",
    "        deleted_paragraph_headings.append(paragraph_heading)\n",
    "        \n",
    "for paragraph_heading in deleted_paragraph_headings:\n",
    "    paragraph_headings.remove(paragraph_heading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, div in enumerate(soup.findAll(attrs={'class': 'js-subsection-body body-content-wrapper'})):\n",
    "    paragraphs.append(div.text)\n",
    "    if(i == 2):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = \"\" #Initialize empty paragraph\n",
    "for j, div in enumerate(soup.findAll(attrs={'class': 'js-subsection-body body-content-wrapper'})):\n",
    "    if(j >= 3): #Start operation from 4th section\n",
    "        for k, tag in enumerate(div.contents): #Loop through the tags in each section\n",
    "            if(k == 0 and tag.name != 'h3'): #Skip section if first tag is not an h3 tag\n",
    "                break\n",
    "            if(tag.name == 'h3' and tag.text != 'Notes'): #If tag is an h3 tag and and not a table notes section\n",
    "                if(p != \"\"):\n",
    "                    paragraphs.append(p) #Append paragraph to list of paragraphs if paragraph is not empty\n",
    "                p = \"\" #Empty paragraph\n",
    "            if(tag.name == 'p' or tag.name == 'ol' or tag.name == 'table'):\n",
    "                p = p + tag.text #Concatenate tag texts if tag is a p, ol or table tag.\n",
    "\n",
    "paragraphs.append(p) #Append last concatenated paragraph to list of paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "62"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "len(paragraph_headings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "62"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "len(paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Paragraph': paragraph_headings, 'Content': paragraphs})\n",
    "df.to_csv('immigration.csv', index=False, encoding='UTF8', header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[WDM] - Current google-chrome version is 83.0.4103\n[WDM] - Get LATEST driver version for 83.0.4103\n[WDM] - Driver [C:\\Users\\Joel\\.wdm\\drivers\\chromedriver\\win32\\83.0.4103.39\\chromedriver.exe] found in cache\n \n"
    }
   ],
   "source": [
    "#Get immigration rules index page\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "driver.get(\"https://www.gov.uk/guidance/immigration-rules/immigration-rules-index\")\n",
    "\n",
    "content = driver.page_source\n",
    "soup = BeautifulSoup(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "\"\"\"Creating a Document Node for the Part 6A: Points-Based System document\"\"\"\n",
    "for doc in soup.findAll('tr'):\n",
    "    if(doc.a):\n",
    "        title = doc.a.text\n",
    "        url = doc.a['href']\n",
    "        tx = graph.begin()\n",
    "        tx.evaluate('''\n",
    "            CREATE (doc:Document {title: $title, url: $url})\n",
    "            ''', parameters = {'title': title, 'url': url})\n",
    "        tx.commit()\n",
    "        documents.append(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[]"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get points based system immigration rules(part 6a) web page\n",
    "driver = webdriver.Chrome(\"C:\\chromedriver_win32\\chromedriver.exe\")\n",
    "driver.get(\"https://www.gov.uk/guidance/immigration-rules/immigration-rules-part-6a-the-points-based-system\")\n",
    "\n",
    "content = driver.page_source\n",
    "soup = BeautifulSoup(content)\n",
    "\n",
    "\"\"\"Creating a Section Node for Tier 4 (General) Student section\"\"\"\n",
    "for section in soup.findAll(attrs={'class': 'js-subsection-title'}):\n",
    "#     if(section.text == \"Tier 4 (General) Student\"):\n",
    "        tx = graph.begin()\n",
    "        tx.evaluate('''\n",
    "            CREATE (sec:Section {title: $title})\n",
    "            ''', parameters = {'title': section.text})\n",
    "        tx.evaluate('''\n",
    "            MATCH (doc:Document), (sec:Section) \n",
    "            WHERE sec.title = $title AND doc.title = \"Part 6A: Points-Based System\"\n",
    "            CREATE (doc)-[: CONTAINS]->(sec)\n",
    "            ''', parameters = {'title': section.text})\n",
    "        tx.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Creating Paragraph Nodes for Paragraphs under the Tier 4 (General) Student section\"\"\"\n",
    "par_content = \"\"\n",
    "for section in soup.findAll(attrs={'class': 'js-openable'}):\n",
    "    if(section.h2.text == \"Tier 4 (General) Student\"): # Focus on Tier 4 (General) Student section\n",
    "        for tag in section.div.contents: # Loop through tags in section\n",
    "            if(tag.name == 'h3' and tag.text != 'Notes'): # If it is an h3 tag and the tag's text is not a Notes section\n",
    "                if(par_content != \"\"):\n",
    "                    tx = graph.begin()\n",
    "                    tx.evaluate('''\n",
    "                        CREATE (par:Paragraph {title: $title, content: $content})\n",
    "                        ''', parameters = {'title': par_title, 'content': par_content})\n",
    "                    tx.evaluate('''\n",
    "                        MATCH (sec:Section), (par:Paragraph) \n",
    "                        WHERE sec.title = \"Tier 4 (General) Student\" AND par.title = $title\n",
    "                        CREATE (sec)-[: CONTAINS]->(par) \n",
    "                        ''', parameters = {'title': par_title})\n",
    "                    tx.commit()\n",
    "                par_title = tag.text\n",
    "            else:\n",
    "                par_content = par_content + tag.text\n",
    "\n",
    "# For last paragraph in the section\n",
    "tx = graph.begin()\n",
    "tx.evaluate('''\n",
    "    CREATE (sec:Paragraph {title: $title, content: $content})\n",
    "    ''', parameters = {'title': par_title, 'content': par_content})\n",
    "tx.evaluate('''\n",
    "    MATCH (sec:Section), (par:Paragraph) \n",
    "    WHERE sec.title = \"Tier 4 (General) Student\" AND par.title = $title\n",
    "    CREATE (sec)-[: CONTAINS]->(par) \n",
    "    ''', parameters = {'title': par_title})\n",
    "tx.commit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}